{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Notebook for inserting time stamps not chronologically into a zarr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zarr\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First greate an empty zarr, which will be build upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"test.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(datetime):\n",
    "    \"\"\"Create a spatial, single time step dataset with two variables.\"\"\"\n",
    "    dims = (\"time\", \"lat\", \"lon\")\n",
    "    time = [pd.to_datetime(datetime)]\n",
    "    w = 4000\n",
    "    h = 2000\n",
    "    lon = np.linspace(0, 4, w)\n",
    "    lat = np.linspace(50, 52, h)\n",
    "    precipitation_var = xr.DataArray(np.random.rand(1, h, w), coords=(time, lat, lon), dims=(\"time\", \"lat\", \"lon\"))\n",
    "    temperature_var = xr.DataArray(np.random.rand(1, h, w), coords=(time, lat, lon), dims=(\"time\", \"lat\", \"lon\"))\n",
    "    ds = xr.Dataset({\"precipitation\": precipitation_var, \"temperature\": temperature_var})\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a single time step dataset with default chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset(\"2018-01-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.time.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.precipitation.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_zarr(DATASET_PATH, mode=\"w\")\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (1,),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " 'units': 'days since 2018-01-01 00:00:00',\n",
       " 'calendar': 'proleptic_gregorian',\n",
       " 'dtype': dtype('int64')}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.time.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (1, 250, 500),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " '_FillValue': nan,\n",
       " 'dtype': dtype('float64')}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.precipitation.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect default chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_group = zarr.open(\"test.zarr\", mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This zarr will only contain time stamps for every other day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 10, 2):\n",
    "    print(i)\n",
    "    ds = create_dataset(f\"2018-01-0{i}\")\n",
    "    for var_name, var_array in root_group.arrays():\n",
    "        var = ds[var_name]\n",
    "        if 'time' in var.dims:            \n",
    "            time_axis = var.dims.index('time')\n",
    "            var_array.append(var, axis=time_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time.encoding{'chunks': (1,),\n",
    "                 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
    "                 'filters': None,\n",
    "                 'units': 'days since 2018-01-01 00:00:00',\n",
    "                 'calendar': 'proleptic_gregorian',\n",
    "                 'dtype': dtype('int64')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unable to decode time units 'days since 2018-01-01 00:00:00' with calendar 'proleptic_gregorian'. Try opening your dataset with decode_times=False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36mdecode_cf_datetime\u001b[0;34m(num_dates, units, calendar, use_cftime)\u001b[0m\n\u001b[1;32m    175\u001b[0m             dates = _decode_datetime_with_pandas(flat_num_dates, units,\n\u001b[0;32m--> 176\u001b[0;31m                                                  calendar)\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36m_decode_datetime_with_pandas\u001b[0;34m(flat_num_dates, units, calendar)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_num_dates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mref_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_num_dates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mref_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/pandas/core/tools/timedeltas.py\u001b[0m in \u001b[0;36mto_timedelta\u001b[0;34m(arg, unit, box, errors)\u001b[0m\n\u001b[1;32m    113\u001b[0m     return _coerce_scalar_to_timedelta_type(arg, unit=unit,\n\u001b[0;32m--> 114\u001b[0;31m                                             box=box, errors=errors)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/pandas/core/tools/timedeltas.py\u001b[0m in \u001b[0;36m_coerce_scalar_to_timedelta_type\u001b[0;34m(r, unit, box, errors)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/timedeltas.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timedeltas.Timedelta.__new__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/timedeltas.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timedeltas.convert_to_timedelta64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/timedeltas.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timedeltas.cast_from_unit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python int too large to convert to C long",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36m_decode_cf_datetime_dtype\u001b[0;34m(data, units, calendar, use_cftime)\u001b[0m\n\u001b[1;32m     93\u001b[0m         result = decode_cf_datetime(example_value, units, calendar,\n\u001b[0;32m---> 94\u001b[0;31m                                     use_cftime)\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36mdecode_cf_datetime\u001b[0;34m(num_dates, units, calendar, use_cftime)\u001b[0m\n\u001b[1;32m    178\u001b[0m             dates = _decode_datetime_with_cftime(\n\u001b[0;32m--> 179\u001b[0;31m                 flat_num_dates.astype(np.float), units, calendar)\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36m_decode_datetime_with_cftime\u001b[0;34m(num_dates, units, calendar)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return np.asarray(cftime.num2date(num_dates, units, calendar,\n\u001b[0;32m--> 113\u001b[0;31m                                           only_use_cftime_datetimes=True))\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcftime/_cftime.pyx\u001b[0m in \u001b[0;36mcftime._cftime.num2date\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcftime/_cftime.pyx\u001b[0m in \u001b[0;36mcftime._cftime.utime.num2date\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcftime/_cftime.pyx\u001b[0m in \u001b[0;36mcftime._cftime.DateFromJulianDay\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcftime/_cftime.pyx\u001b[0m in \u001b[0;36mcftime._cftime.DateFromJulianDay.getdateinfo\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcftime/_cftime.pyx\u001b[0m in \u001b[0;36mcftime._cftime._IntJulianDayToDate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: julian day must be a positive integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-37cf141ebc8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.zarr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mopen_zarr\u001b[0;34m(store, group, synchronizer, auto_chunk, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated)\u001b[0m\n\u001b[1;32m    446\u001b[0m                                       \u001b[0msynchronizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                                       group=group, consolidated=consolidated)\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_decode_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzarr_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;31m# auto chunking needs to be here and not in ZarrStore because variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mmaybe_decode_store\u001b[0;34m(store, lock)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mconcat_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_coords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_coords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             drop_variables=drop_variables)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# TODO: this is where we would apply caching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf\u001b[0;34m(obj, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime)\u001b[0m\n\u001b[1;32m    477\u001b[0m     vars, attrs, coord_names = decode_cf_variables(\n\u001b[1;32m    478\u001b[0m         \u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         decode_coords, drop_variables=drop_variables, use_cftime=use_cftime)\n\u001b[0m\u001b[1;32m    480\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_coords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf_variables\u001b[0;34m(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             stack_char_dim=stack_char_dim, use_cftime=use_cftime)\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecode_coords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mvar_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf_variable\u001b[0;34m(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime)\u001b[0m\n\u001b[1;32m    304\u001b[0m         for coder in [times.CFTimedeltaCoder(),\n\u001b[1;32m    305\u001b[0m                       times.CFDatetimeCoder(use_cftime=use_cftime)]:\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     dimensions, data, attributes, encoding = (\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, variable, name)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mcalendar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'calendar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             dtype = _decode_cf_datetime_dtype(data, units, calendar,\n\u001b[0;32m--> 419\u001b[0;31m                                               self.use_cftime)\n\u001b[0m\u001b[1;32m    420\u001b[0m             transform = partial(\n\u001b[1;32m    421\u001b[0m                 \u001b[0mdecode_cf_datetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalendar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalendar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xcube/lib/python3.7/site-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36m_decode_cf_datetime_dtype\u001b[0;34m(data, units, calendar, use_cftime)\u001b[0m\n\u001b[1;32m     99\u001b[0m                \u001b[0;34m'opening your dataset with decode_times=False.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                % (units, calendar_msg))\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unable to decode time units 'days since 2018-01-01 00:00:00' with calendar 'proleptic_gregorian'. Try opening your dataset with decode_times=False."
     ]
    }
   ],
   "source": [
    "ds = xr.open_zarr(\"test.zarr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another dataset which contains only one time stamp and will be appended to the first data set, stored in test.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SINGLE_PATH = \"test_single.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset(\"2018-01-01\")\n",
    "ds.to_zarr(DATASET_SINGLE_PATH, mode=\"w\")\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_group = zarr.open(\"test_single.zarr\", mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4, 9, 2):\n",
    "#     print(i)\n",
    "#     ds = create_dataset(f\"2018-01-0{i}\")\n",
    "#     for var_name, var_array in root_group.arrays():\n",
    "#         var = ds[var_name]\n",
    "#         if 'time' in var.dims:            \n",
    "#             time_axis = var.dims.index('time')\n",
    "#             var_array.append(var, axis=time_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset(f\"2018-01-04\")\n",
    "for var_name, var_array in root_group.arrays():\n",
    "    var = ds[var_name]\n",
    "    if 'time' in var.dims:            \n",
    "        time_axis = var.dims.index('time')\n",
    "        var_array.append(var, axis=time_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(DATASET_PATH, decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_single = xr.open_zarr(DATASET_SINGLE_PATH, decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()\n",
    "ds_single.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time stamp of both data sets is the same, in order to have the same time encoding. So in this case we are interested in the second time stamp of the ds_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_merge_or_append(src_path, dst_path):\n",
    "    \"\"\"Check whether the time stamp of the current input file is before or past the last time stamp of the created data cube\"\"\"\n",
    "    ds_single = xr.open_zarr(src_path, decode_times=False)\n",
    "    ds = xr.open_zarr(dst_path, decode_times=False)\n",
    "    if (np.greater(ds_single.time.values, ds.time[-1].values)).all() == True:\n",
    "        #append modus\n",
    "        print(\"Append modus is chosen.\")\n",
    "    else:\n",
    "        check_if_unique(src_path, dst_path)\n",
    "        print(\"Merge modus is chosen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_unique(src_path, dst_path):\n",
    "    \"\"\"Check if to be added time stamp is unique \"\"\"\n",
    "    ds_single = xr.open_zarr(src_path, decode_times=False)\n",
    "    ds = xr.open_zarr(dst_path, decode_times=False)\n",
    "    for src_idx in range(ds_single.time.shape[0]):\n",
    "        mask = (np.greater(ds.time.values, ds_single.time[src_idx].values)==False)&(np.equal(ds.time.values, ds_single.time[src_idx].values)==True)\n",
    "        if mask.all()==False:\n",
    "            print(\"merging\")\n",
    "            merge_single_zarr_into_destination_zarr(src_path, dst_path, src_idx)\n",
    "        else:\n",
    "            print(\"All timestamps to be merged are aleady in destination data set, and are skipped.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_above(array, target):\n",
    "    diff = array - target\n",
    "    mask = np.ma.less_equal(diff, 0)\n",
    "    # We need to mask the negative differences and zero\n",
    "    # since we are looking for values above\n",
    "    if np.all(mask):\n",
    "        return None # returns None if target is greater than any value\n",
    "    masked_diff = np.ma.masked_array(diff, mask)\n",
    "    return masked_diff.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_directory(path_to_ds, old_index, new_time_i):\n",
    "    ds = xr.open_zarr(path_to_ds, decode_times=False)\n",
    "    for v in ds.variables:\n",
    "        if (v != 'lat') and (v != 'lon'):\n",
    "            path = os.path.join(path_to_ds, v)\n",
    "            for root, dirs, files in os.walk(path):  \n",
    "                for filename in files:\n",
    "                    if (str(old_index)) in filename[0] and (v != \"time\"):\n",
    "                        parts = filename.split('.',1)\n",
    "                        new_name = (str(new_time_i) + '.{}').format(parts[1]) \n",
    "                        if new_name != path:\n",
    "                             os.rename(os.path.join(path, filename), os.path.join(path, new_name))\n",
    "                    elif (str(old_index)) in filename[0] and (v == \"time\"):\n",
    "                        if str(new_time_i) != path:\n",
    "                            os.rename(os.path.join(path, filename), os.path.join(path, str(new_time_i)))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_zarray(dst_path, variable, line_to_adjust):\n",
    "    with open((os.path.join(dst_path, variable, '.zarray')), 'r') as zarray:\n",
    "        data = zarray.readlines()\n",
    "    position = 8\n",
    "    white_space = len(data[line_to_adjust])-1\n",
    "    data[line_to_adjust] = (str(int(data[line_to_adjust][position])+1) + data[line_to_adjust][9:]).rjust(len(data[line_to_adjust][position])+white_space)\n",
    "\n",
    "    with open((os.path.join(dst_path, variable, '.zarray')), 'w') as zarray:\n",
    "        zarray.writelines(data)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_into_target(src_path, dst_path, src_index):\n",
    "    ds = xr.open_zarr(src_path, decode_times=False)\n",
    "    for v in ds.variables:\n",
    "        if (v != 'lat') and (v != 'lon'):\n",
    "            path = os.path.join(src_path, v)\n",
    "            for root, dirs, files in os.walk(path):  \n",
    "                for filename in files:\n",
    "                    if str(src_index) in filename[0]:\n",
    "                        copyfile((os.path.join(src_path, v, filename)), (os.path.join(dst_path, v,  filename)))\n",
    "            if v != \"time\":\n",
    "                line_to_adjust = 18\n",
    "            elif v == \"time\":\n",
    "                line_to_adjust = 16\n",
    "            adjust_zarray(dst_path, v, line_to_adjust)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_single_zarr_into_destination_zarr(src_path, dst_path, old_idx):\n",
    "    ds_single = xr.open_zarr(src_path, decode_times=False)\n",
    "    ds = xr.open_zarr(dst_path, decode_times=False)\n",
    "    new_idx = find_nearest_above(ds.time.values, ds_single.time[old_idx].values)\n",
    "    ds.close()\n",
    "    ds_single.close()\n",
    "# Preparing the source directory with the single time stamp to be ready for merging \n",
    "# --> files of variables, excluding \"lat\" and \"lon\" need to be renamed\n",
    "    rename_directory(src_path, old_idx, new_idx)\n",
    "# Preparing the destinanation directory to be ready for single time stam to be merged \n",
    "# --> files of variables, excluding \"lat\" and \"lon\" need to be renamed\n",
    "# The renaming needs to happen in reversed order and starting at the index of nearest above value:\n",
    "    for i in reversed(range(new_idx,ds.time.shape[0])):\n",
    "        rename_directory(dst_path, i, (i +1))\n",
    "# Final step: copy the single time stamp files into the destination zarr and adjusting .zarray to the change. \n",
    "    copy_into_target(src_path, dst_path, new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_merge_or_append(DATASET_SINGLE_PATH,DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open((os.path.join(DATASET_PATH, \"temperature\", '.zarray')), 'r') as zarray:\n",
    "#           data = zarray.readlines()\n",
    "# data[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (str(int(data[18][8])+1) + data[18][9:]).rjust(len(data[18][8])+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open((os.path.join(DATASET_PATH, \"time\", '.zarray')), 'r') as zarray:\n",
    "#           data = zarray.readlines()\n",
    "# data[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (data[16][len(data[line_to_adjust])-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data[16])-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[18][9:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_to_adjust = 16\n",
    "# position = 8\n",
    "# white_space = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (str(int(data[line_to_adjust][position])+1) + data[line_to_adjust][9:]).rjust(len(data[line_to_adjust][position])+white_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (str(int(data[line_to_adjust][position])+1) + data[line_to_adjust][9:]).rjust(len(data[line_to_adjust][position])+white_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data[line_to_adjust])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(DATASET_SINGLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
