{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Notebook for inserting time stamps not chronologically into a zarr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zarr\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First greate an empty zarr, which will be build upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"test.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(datetime):\n",
    "    \"\"\"Create a spatial, single time step dataset with two variables.\"\"\"\n",
    "    dims = (\"time\", \"lat\", \"lon\")\n",
    "    time = [pd.to_datetime(datetime)]\n",
    "    w = 4000\n",
    "    h = 2000\n",
    "    lon = np.linspace(0, 4, w)\n",
    "    lat = np.linspace(50, 52, h)\n",
    "    precipitation_var = xr.DataArray(np.random.rand(1, h, w), coords=(time, lat, lon), dims=(\"time\", \"lat\", \"lon\"))\n",
    "    temperature_var = xr.DataArray(np.random.rand(1, h, w), coords=(time, lat, lon), dims=(\"time\", \"lat\", \"lon\"))\n",
    "    ds = xr.Dataset({\"precipitation\": precipitation_var, \"temperature\": temperature_var})\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a single time step dataset with default chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset(\"2018-01-01\")\n",
    "ds.to_zarr(DATASET_PATH, mode=\"w\")\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect default chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_group = zarr.open(\"test.zarr\", mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This zarr will only contain time stamps for every other day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 7, 2):\n",
    "    print(i)\n",
    "    ds = create_dataset(f\"2018-01-0{i}\")\n",
    "    for var_name, var_array in root_group.arrays():\n",
    "        var = ds[var_name]\n",
    "        if 'time' in var.dims:            \n",
    "            time_axis = var.dims.index('time')\n",
    "            var_array.append(var, axis=time_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another dataset which contains only one time stamp and will be appended to the first data set, stored in test.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SINGLE_PATH = \"test_single.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset(\"2018-01-01\")\n",
    "ds.to_zarr(DATASET_SINGLE_PATH, mode=\"w\")\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_group = zarr.open(\"test_single.zarr\", mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4, 9, 2):\n",
    "#     print(i)\n",
    "#     ds = create_dataset(f\"2018-01-0{i}\")\n",
    "#     for var_name, var_array in root_group.arrays():\n",
    "#         var = ds[var_name]\n",
    "#         if 'time' in var.dims:            \n",
    "#             time_axis = var.dims.index('time')\n",
    "#             var_array.append(var, axis=time_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset(f\"2018-01-04\")\n",
    "for var_name, var_array in root_group.arrays():\n",
    "    var = ds[var_name]\n",
    "    if 'time' in var.dims:            \n",
    "        time_axis = var.dims.index('time')\n",
    "        var_array.append(var, axis=time_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(DATASET_PATH, decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_single = xr.open_zarr(DATASET_SINGLE_PATH, decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()\n",
    "ds_single.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time stamp of both data sets is the same, in order to have the same time encoding. So in this case we are interested in the second time stamp of the ds_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_merge_or_append(src_path, dst_path):\n",
    "    \"\"\"Check whether the time stamp of the current input file is before or past the last time stamp of the created data cube\"\"\"\n",
    "    ds_single = xr.open_zarr(src_path, decode_times=False)\n",
    "    ds = xr.open_zarr(dst_path, decode_times=False)\n",
    "    if (np.greater(ds_single.time.values, ds.time[-1].values)).all() == True:\n",
    "        #append modus\n",
    "        print(\"Append modus is chosen.\")\n",
    "    else:\n",
    "        check_if_unique(src_path, dst_path)\n",
    "        print(\"Merge modus is chosen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_unique(src_path, dst_path):\n",
    "    \"\"\"Check if to be added time stamp is unique \"\"\"\n",
    "    ds_single = xr.open_zarr(src_path, decode_times=False)\n",
    "    ds = xr.open_zarr(dst_path, decode_times=False)\n",
    "    for src_idx in range(ds_single.time.shape[0]):\n",
    "        mask = (np.greater(ds.time.values, ds_single.time[src_idx].values)==False)&(np.equal(ds.time.values, ds_single.time[src_idx].values)==True)\n",
    "        if mask.all()==False:\n",
    "            print(\"merging\")\n",
    "            merge_single_zarr_into_destination_zarr(src_path, dst_path, src_idx)\n",
    "        else:\n",
    "            print(\"All timestamps to be merged are aleady in destination data set, and are skipped.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_above(array, target):\n",
    "    diff = array - target\n",
    "    mask = np.ma.less_equal(diff, 0)\n",
    "    # We need to mask the negative differences and zero\n",
    "    # since we are looking for values above\n",
    "    if np.all(mask):\n",
    "        return None # returns None if target is greater than any value\n",
    "    masked_diff = np.ma.masked_array(diff, mask)\n",
    "    return masked_diff.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_directory(path_to_ds, old_index, new_time_i):\n",
    "    ds = xr.open_zarr(path_to_ds, decode_times=False)\n",
    "    for v in ds.variables:\n",
    "        if (v != 'lat') and (v != 'lon'):\n",
    "            path = os.path.join(path_to_ds, v)\n",
    "            for root, dirs, files in os.walk(path):  \n",
    "                for filename in files:\n",
    "                    if (str(old_index)) in filename[0] and (v != \"time\"):\n",
    "                        parts = filename.split('.',1)\n",
    "                        new_name = (str(new_time_i) + '.{}').format(parts[1]) \n",
    "                        if new_name != path:\n",
    "                             os.rename(os.path.join(path, filename), os.path.join(path, new_name))\n",
    "                    elif (str(old_index)) in filename[0] and (v == \"time\"):\n",
    "                        if str(new_time_i) != path:\n",
    "                            os.rename(os.path.join(path, filename), os.path.join(path, str(new_time_i)))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_zarray(dst_path, variable, line_to_adjust):\n",
    "    with open((os.path.join(dst_path, variable, '.zarray')), 'r') as zarray:\n",
    "        data = zarray.readlines()\n",
    "    position = 8\n",
    "    white_space = len(data[line_to_adjust])-1\n",
    "    data[line_to_adjust] = (str(int(data[line_to_adjust][position])+1) + data[line_to_adjust][9:]).rjust(len(data[line_to_adjust][position])+white_space)\n",
    "\n",
    "    with open((os.path.join(dst_path, variable, '.zarray')), 'w') as zarray:\n",
    "        zarray.writelines(data)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_into_target(src_path, dst_path, src_index):\n",
    "    ds = xr.open_zarr(src_path, decode_times=False)\n",
    "    for v in ds.variables:\n",
    "        if (v != 'lat') and (v != 'lon'):\n",
    "            path = os.path.join(src_path, v)\n",
    "            for root, dirs, files in os.walk(path):  \n",
    "                for filename in files:\n",
    "                    if str(src_index) in filename[0]:\n",
    "                        copyfile((os.path.join(src_path, v, filename)), (os.path.join(dst_path, v,  filename)))\n",
    "            if v != \"time\":\n",
    "                line_to_adjust = 18\n",
    "            elif v == \"time\":\n",
    "                line_to_adjust = 16\n",
    "            adjust_zarray(dst_path, v, line_to_adjust)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_single_zarr_into_destination_zarr(src_path, dst_path, old_idx):\n",
    "    ds_single = xr.open_zarr(src_path, decode_times=False)\n",
    "    ds = xr.open_zarr(dst_path, decode_times=False)\n",
    "    new_idx = find_nearest_above(ds.time.values, ds_single.time[old_idx].values)\n",
    "    ds.close()\n",
    "    ds_single.close()\n",
    "# Preparing the source directory with the single time stamp to be ready for merging \n",
    "# --> files of variables, excluding \"lat\" and \"lon\" need to be renamed\n",
    "    rename_directory(src_path, old_idx, new_idx)\n",
    "# Preparing the destinanation directory to be ready for single time stam to be merged \n",
    "# --> files of variables, excluding \"lat\" and \"lon\" need to be renamed\n",
    "# The renaming needs to happen in reversed order and starting at the index of nearest above value:\n",
    "    for i in reversed(range(new_idx,ds.time.shape[0])):\n",
    "        rename_directory(dst_path, i, (i +1))\n",
    "# Final step: copy the single time stamp files into the destination zarr and adjusting .zarray to the change. \n",
    "    copy_into_target(src_path, dst_path, new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging\n",
      "merging\n",
      "Merge modus is chosen.\n",
      "CPU times: user 618 ms, sys: 793 ms, total: 1.41 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "check_merge_or_append(DATASET_SINGLE_PATH,DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open((os.path.join(DATASET_PATH, \"temperature\", '.zarray')), 'r') as zarray:\n",
    "#           data = zarray.readlines()\n",
    "# data[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (str(int(data[18][8])+1) + data[18][9:]).rjust(len(data[18][8])+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open((os.path.join(DATASET_PATH, \"time\", '.zarray')), 'r') as zarray:\n",
    "#           data = zarray.readlines()\n",
    "# data[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (data[16][len(data[line_to_adjust])-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data[16])-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[18][9:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_to_adjust = 16\n",
    "# position = 8\n",
    "# white_space = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (str(int(data[line_to_adjust][position])+1) + data[line_to_adjust][9:]).rjust(len(data[line_to_adjust][position])+white_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (str(int(data[line_to_adjust][position])+1) + data[line_to_adjust][9:]).rjust(len(data[line_to_adjust][position])+white_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data[line_to_adjust])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(DATASET_SINGLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
